{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MLMI2-CSSI/foundry/blob/main/examples/02_working_with_data/working_with_data.ipynb)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Foundry Data\n",
    "\n",
    "**Time:** 15 minutes  \n",
    "**Prerequisites:** Completed quickstart  \n",
    "**What you'll learn:**\n",
    "- Understanding dataset schemas\n",
    "- Loading specific splits\n",
    "- Using data with PyTorch and TensorFlow\n",
    "- Working with different data types\n",
    "- JSON output for programmatic access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow>=16.1.0\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl (34.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 15.0.0\n",
      "    Uninstalling pyarrow-15.0.0:\n",
      "      Successfully uninstalled pyarrow-15.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "taipy-core 3.1.1 requires pandas<=2.2.0,>=1.3.5, but you have pandas 2.3.1 which is incompatible.\n",
      "taipy-core 3.1.1 requires pyarrow<=15.0.0,>=14.0.2, but you have pyarrow 22.0.0 which is incompatible.\n",
      "streamlit 1.36.0 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.36.0 requires rich<14,>=10.14.0, but you have rich 14.2.0 which is incompatible.\n",
      "datasets 2.19.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyarrow-22.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"pyarrow>=16.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundry import Foundry\n",
    "\n",
    "# HTTPS download is now the default\n",
    "f = Foundry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Dataset Schemas\n",
    "\n",
    "Before loading data, understand what's in it using `get_schema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataset\n",
    "results = f.search(\"band gap\", limit=1)\n",
    "dataset = results.iloc[0].FoundryDataset\n",
    "\n",
    "# Get the schema\n",
    "schema = dataset.get_schema()\n",
    "\n",
    "print(f\"Dataset: {schema['name']}\")\n",
    "print(f\"Title: {schema['title']}\")\n",
    "print(f\"DOI: {schema['doi']}\")\n",
    "print(f\"Data Type: {schema['data_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine fields (columns)\n",
    "print(\"Fields:\")\n",
    "print(\"-\" * 60)\n",
    "for field in schema['fields']:\n",
    "    role = field['role']  # 'input' or 'target'\n",
    "    name = field['name']\n",
    "    desc = field['description'] or 'No description'\n",
    "    units = field['units'] or ''\n",
    "    print(f\"  [{role:6}] {name}: {desc} {f'({units})' if units else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine splits (train/test/validation)\n",
    "print(\"Splits:\")\n",
    "print(\"-\" * 60)\n",
    "for split in schema['splits']:\n",
    "    print(f\"  - {split['name']}: {split.get('type', 'data')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Specific Splits\n",
    "\n",
    "Load only the data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Download of: https://data.materialsdatafacility.org/foundry/foundry_g4mp2_solvation_v1.2/g4mp2_data.json\n",
      "Downloading... 206.19 MBTraining data keys: dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "# Load only training data\n",
    "train_data = dataset.get_as_dict(split='train')\n",
    "print(f\"Training data keys: {train_data.keys() if isinstance(train_data, dict) else type(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All splits: ['train']\n"
     ]
    }
   ],
   "source": [
    "# Load all splits at once\n",
    "all_data = dataset.get_as_dict()\n",
    "print(f\"All splits: {list(all_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading with Schema Information\n",
    "\n",
    "Use `include_schema=True` to get data AND metadata together. This is especially useful for programmatic/agent workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data with schema attached\n",
    "result = dataset.get_as_dict(include_schema=True)\n",
    "\n",
    "print(f\"Result keys: {result.keys()}\")\n",
    "print(f\"\\nSchema name: {result['schema']['name']}\")\n",
    "print(f\"Data splits: {list(result['data'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as a PyTorch Dataset\n",
    "try:\n",
    "    torch_dataset = dataset.get_as_torch(split='train')\n",
    "    \n",
    "    # Use with DataLoader\n",
    "    from torch.utils.data import DataLoader\n",
    "    loader = DataLoader(torch_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = next(iter(loader))\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    print(f\"Batch size: {len(batch[0]) if isinstance(batch, tuple) else batch.shape[0]}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Install with: pip install torch\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load as PyTorch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TensorFlow Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as a TensorFlow Dataset\n",
    "try:\n",
    "    tf_dataset = dataset.get_as_tensorflow(split='train')\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    tf_dataset = tf_dataset.batch(32).prefetch(1)\n",
    "    \n",
    "    # Get a batch\n",
    "    for batch in tf_dataset.take(1):\n",
    "        print(f\"Batch type: {type(batch)}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load as TensorFlow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. JSON Output for Programmatic Access\n",
    "\n",
    "Use `as_json=True` for agent-friendly output (lists of dicts instead of DataFrames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with JSON output\n",
    "# as_json=True returns a list of dicts instead of a DataFrame\n",
    "results_json = f.search(\"band gap\", limit=3, as_json=True)\n",
    "\n",
    "print(f\"Type: {type(results_json)}\")\n",
    "print(f\"Number of results: {len(results_json)}\")\n",
    "\n",
    "for ds in results_json:\n",
    "    print(f\"\\n- {ds['name']}\")\n",
    "    print(f\"  Title: {ds['title']}\")\n",
    "    print(f\"  DOI: {ds['doi']}\")\n",
    "    print(f\"  Fields: {ds.get('fields', [])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datasets as JSON\n",
    "import json\n",
    "\n",
    "all_datasets = f.list(limit=5, as_json=True)\n",
    "print(json.dumps(all_datasets[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Browsing the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available datasets\n",
    "catalog = f.list(limit=10)\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific dataset by DOI\n",
    "# Replace with a real DOI from your search results\n",
    "# dataset = f.get_dataset(\"10.18126/xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Working with HDF5 Data\n",
    "\n",
    "Some datasets use HDF5 format for large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data keeping HDF5 format (for very large datasets)\n",
    "# data_hdf5 = dataset.get_as_dict(as_hdf5=True)\n",
    "# This returns h5py objects that load lazily\n",
    "print(\"Use as_hdf5=True for lazy loading of large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Method | Use Case |\n",
    "|--------|----------|\n",
    "| `get_schema()` | Understand dataset structure before loading |\n",
    "| `get_as_dict()` | General purpose loading |\n",
    "| `get_as_dict(split='train')` | Load specific split |\n",
    "| `get_as_dict(include_schema=True)` | Data + metadata together |\n",
    "| `get_as_torch()` | PyTorch DataLoader compatible |\n",
    "| `get_as_tensorflow()` | tf.data.Dataset compatible |\n",
    "| `f.search(as_json=True)` | Programmatic/agent access |\n",
    "\n",
    "**Next:** See `03_advanced_workflows.ipynb` for publishing, CLI, and agent integration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}