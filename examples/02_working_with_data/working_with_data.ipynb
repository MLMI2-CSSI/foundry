{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Foundry Data\n",
    "\n",
    "**Time:** 15 minutes  \n",
    "**Prerequisites:** Completed quickstart  \n",
    "**What you'll learn:**\n",
    "- Understanding dataset schemas\n",
    "- Loading specific splits\n",
    "- Using data with PyTorch and TensorFlow\n",
    "- Working with different data types\n",
    "- JSON output for programmatic access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from foundry import Foundry\n\n# HTTPS download is now the default\nf = Foundry()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Dataset Schemas\n",
    "\n",
    "Before loading data, understand what's in it using `get_schema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get a dataset\nresults = f.search(\"band gap\", limit=1)\ndataset = results.iloc[0].FoundryDataset\n\n# Get the schema\nschema = dataset.get_schema()\n\nprint(f\"Dataset: {schema['name']}\")\nprint(f\"Title: {schema['title']}\")\nprint(f\"DOI: {schema['doi']}\")\nprint(f\"Data Type: {schema['data_type']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine fields (columns)\nprint(\"Fields:\")\nprint(\"-\" * 60)\nfor field in schema['fields']:\n    role = field['role']  # 'input' or 'target'\n    name = field['name']\n    desc = field['description'] or 'No description'\n    units = field['units'] or ''\n    print(f\"  [{role:6}] {name}: {desc} {f'({units})' if units else ''}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine splits (train/test/validation)\nprint(\"Splits:\")\nprint(\"-\" * 60)\nfor split in schema['splits']:\n    print(f\"  - {split['name']}: {split.get('type', 'data')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Specific Splits\n",
    "\n",
    "Load only the data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Download of: https://data.materialsdatafacility.org/foundry/foundry_g4mp2_solvation_v1.2/g4mp2_data.json\n",
      "Downloading... 206.19 MBTraining data keys: dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "# Load only training data\n",
    "train_data = dataset.get_as_dict(split='train')\n",
    "print(f\"Training data keys: {train_data.keys() if isinstance(train_data, dict) else type(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All splits: ['train']\n"
     ]
    }
   ],
   "source": [
    "# Load all splits at once\n",
    "all_data = dataset.get_as_dict()\n",
    "print(f\"All splits: {list(all_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading with Schema Information\n",
    "\n",
    "Use `include_schema=True` to get data AND metadata together. This is especially useful for programmatic/agent workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get data with schema attached\nresult = dataset.get_as_dict(include_schema=True)\n\nprint(f\"Result keys: {result.keys()}\")\nprint(f\"\\nSchema name: {result['schema']['name']}\")\nprint(f\"Data splits: {list(result['data'].keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as a PyTorch Dataset\n",
    "try:\n",
    "    torch_dataset = dataset.get_as_torch(split='train')\n",
    "    \n",
    "    # Use with DataLoader\n",
    "    from torch.utils.data import DataLoader\n",
    "    loader = DataLoader(torch_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = next(iter(loader))\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    print(f\"Batch size: {len(batch[0]) if isinstance(batch, tuple) else batch.shape[0]}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Install with: pip install torch\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load as PyTorch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TensorFlow Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as a TensorFlow Dataset\n",
    "try:\n",
    "    tf_dataset = dataset.get_as_tensorflow(split='train')\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    tf_dataset = tf_dataset.batch(32).prefetch(1)\n",
    "    \n",
    "    # Get a batch\n",
    "    for batch in tf_dataset.take(1):\n",
    "        print(f\"Batch type: {type(batch)}\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load as TensorFlow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. JSON Output for Programmatic Access\n",
    "\n",
    "Use `as_json=True` for agent-friendly output (lists of dicts instead of DataFrames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search with JSON output\n# as_json=True returns a list of dicts instead of a DataFrame\nresults_json = f.search(\"band gap\", limit=3, as_json=True)\n\nprint(f\"Type: {type(results_json)}\")\nprint(f\"Number of results: {len(results_json)}\")\n\nfor ds in results_json:\n    print(f\"\\n- {ds['name']}\")\n    print(f\"  Title: {ds['title']}\")\n    print(f\"  DOI: {ds['doi']}\")\n    print(f\"  Fields: {ds.get('fields', [])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List all datasets as JSON\nimport json\n\nall_datasets = f.list(limit=5, as_json=True)\nprint(json.dumps(all_datasets[0], indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Browsing the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available datasets\n",
    "catalog = f.list(limit=10)\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific dataset by DOI\n",
    "# Replace with a real DOI from your search results\n",
    "# dataset = f.get_dataset(\"10.18126/xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Working with HDF5 Data\n",
    "\n",
    "Some datasets use HDF5 format for large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data keeping HDF5 format (for very large datasets)\n",
    "# data_hdf5 = dataset.get_as_dict(as_hdf5=True)\n",
    "# This returns h5py objects that load lazily\n",
    "print(\"Use as_hdf5=True for lazy loading of large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Method | Use Case |\n",
    "|--------|----------|\n",
    "| `get_schema()` | Understand dataset structure before loading |\n",
    "| `get_as_dict()` | General purpose loading |\n",
    "| `get_as_dict(split='train')` | Load specific split |\n",
    "| `get_as_dict(include_schema=True)` | Data + metadata together |\n",
    "| `get_as_torch()` | PyTorch DataLoader compatible |\n",
    "| `get_as_tensorflow()` | tf.data.Dataset compatible |\n",
    "| `f.search(as_json=True)` | Programmatic/agent access |\n",
    "\n",
    "**Next:** See `03_advanced_workflows.ipynb` for publishing, CLI, and agent integration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}